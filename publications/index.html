<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="tfpoUs6D1dBmYWqASPBVXGYN-HXRZNU"> <meta name="msvalidate.01" content="8f0b2726a7802e4b90d365a55fbf528b"> <meta name="baidu-site-verification" content="codeva-VhAM0AReUZ"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Dr. Zhiwei Li - Homepage</title> <meta name="author" content="Zhiwei Li "> <meta name="description" content=""> <meta name="keywords" content="ÊùéÂøó‰ºü, Zhiwei Li, UNIST, Ulsan National Institute of Science and Technology, Remote Sensing, Geospatial Intelligence, Cloudy and Rainy Environments, Cloud Detection and Removal, Land Cover &amp; Land Use, Flood"> <meta property="og:site_name" content="Dr. Zhiwei Li - Homepage"> <meta property="og:type" content="website"> <meta property="og:title" content="Dr. Zhiwei Li - Homepage | Publications"> <meta property="og:url" content="https://zhiweili.net//publications/"> <meta property="og:description" content=""> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Publications"> <meta name="twitter:description" content=""> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Zhiwei Li  "
        },
        "url": "https://zhiweili.net//publications/",
        "@type": "WebSite",
        "description": "",
        "headline": "Publications",
        "sameAs": ["https://scholar.google.com/citations?user=SlXpfWMAAAAJ", "https://www.researchgate.net/profile/Zhiwei-Li-21", "https://github.com/dr-lizhiwei", "https://orcid.org/0000-0001-5635-8499", "https://www.linkedin.com/in/dr-li-zhiwei"],
        "name": "Zhiwei Li  ",
        "@context": "https://schema.org"
    }
    </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?c1464bb1aedcdd342d970bc0160ab2fc"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/webfavicon.png?44a0a843911b668a0c90c17d3d67dac0"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhiweili.net//publications/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Dr. Zhiwei Li - Homepage</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News</a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching</a> </li> <li class="nav-item "> <a class="nav-link" href="/resources/">Resources</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/blog/">Blog</a> <a class="dropdown-item" href="/people/">People</a> <a class="dropdown-item" href="/positions/">Open Positions</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title" style="font-size: 24px;">Publications</h1> <p class="post-description"></p> </header> <article> <h5 id="google-scholar-----researchgate-----publication-list"> <a href="https://scholar.google.com/citations?user=SlXpfWMAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><u><b>Google Scholar</b></u></a> ¬†¬†¬†¬†¬†¬† <a href="https://www.researchgate.net/profile/Zhiwei_Li22" rel="external nofollow noopener" target="_blank"><u><b>ResearchGate</b></u></a> ¬†¬†¬†¬†¬†¬† <a href="/publication%20list/"><u><b>Publication List</b></u></a> </h5> <h6><br></h6> <p><strong>Ten featured journal papers</strong> (* corresponding author, # co-first author):</p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2025.11_JAG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2025.11_JAG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2025.11_JAG-1400.webp"></source> <img src="/assets/img/publication_preview/2025.11_JAG.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2025.11_JAG.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Li2025" class="col-sm-8"> <div class="title">TerrainFloodSense: Improving seamless flood mapping with cloudy satellite imagery via water occurrence and terrain data fusion</div> <div class="author"> <em>Zhiwei Li</em>,¬†Shaofen Xu,¬†and¬†Qihao Weng</div> <div class="periodical"> <em>International Journal of Applied Earth Observation and Geoinformation</em>, Nov 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.jag.2025.104855" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://zhiweili.net/assets/pdf/2025.11_JAG_TerrainFloodSense%EF%BC%9AImproving%20seamless%20flood%20mapping%20with%20cloudy%20satellite%20imagery%20via%20water%20occurrence%20and%20terrain%20data%20fusion.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/dr-lizhiwei/TerrainFloodSense" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.jag.2025.104855" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <span class="altmetric-embed" data-badge-type="2" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-popover="right" data-link-target="true" data-doi="10.1016/j.jag.2025.104855"></span> </div> <div class="abstract hidden"> <p>Extreme flood disasters are intensified by climate change, exposing an increasing share of the global population to flood hazards. Accurate monitoring of inundation extents during floods is crucial for disaster management and impact assessment. While remote sensing can provide strong support for flood monitoring, optical satellite images often face significant challenges due to weather conditions and infrequent revisits, particularly in cloudy and rainy regions. To address this limitation and achieve seamless flood mapping with cloudy satellite images, this paper proposes TerrainFloodSense, a novel method that fuses water occurrence with terrain data to enhance the reconstruction of cloud-covered flooding areas, especially under extreme and unprecedented flood scenarios. Specifically, TerrainFloodSense first generates enhanced water occurrence data by Bayesian fusion of terrain indices, including Digital Surface Model (DSM) along with Height Above the Nearest Drainage (HAND), and historical water occurrence data. Then, enhanced water occurrence data are used to fill gaps caused by clouds in water maps derived from optical satellite images, guided by the submaximal stability assumption. The basic idea is that prior terrain information can be incorporated into the initial water occurrence data to enhance the ability to predict the inundation probabilities for both regular pre-flood water and extreme floodwater and to help reconstruction of cloud-covered flooding areas even under extreme flooding scenarios. Simulated experiments and applications in large-area flood mapping cases confirmed that TerrainFloodSense significantly outperformed existing methods, achieving absolute accuracy improvements of 2.95%‚Äì8.86% in overall accuracy and 0.038‚Äì0.087 increases in F1-Score under extreme flooding scenarios. This study demonstrated that the fusion of water occurrence and terrain data can effectively improve seamless flood mapping by using optical satellite images, supporting disaster monitoring and impact assessment in cloudy and rainy environments. The code associated with this study has been made publicly accessible via https://github.com/RCAIG/TerrainFloodSense.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2024.11_npj%20Urban%20Sustainability-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2024.11_npj%20Urban%20Sustainability-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2024.11_npj%20Urban%20Sustainability-1400.webp"></source> <img src="/assets/img/publication_preview/2024.11_npj%20Urban%20Sustainability.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2024.11_npj Urban Sustainability.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Weng2024" class="col-sm-8"> <div class="title">How will ai transform urban observing, sensing, imaging, and mapping?</div> <div class="author"> Qihao Weng,¬†<em>Zhiwei Li</em>,¬†Yinxai Cao,¬†Xiaoyan Lu,¬†Paolo Gamba,¬†Xiaoxiang Zhu,¬†Yonghao Xu,¬†Fan Zhang,¬†Rongjun Qin,¬†Micheal. Y. Yang, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Peifeng Ma, Wei Huang, Tiangang Yin, Qiming Zheng, Yuhan Zhou, Greg Asner' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>npj Urban Sustainability</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1038/s42949-024-00188-3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://zhiweili.net/assets/pdf/2024.11_npj%20Urban%20Sustainability_How%20will%20AI%20transform%20urban%20observing,%20sensing,%20imaging,%20and%20mapping.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1038/s42949-024-00188-3" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <span class="altmetric-embed" data-badge-type="2" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-popover="right" data-link-target="true" data-doi="10.1038/s42949-024-00188-3"></span> </div> <div class="abstract hidden"> <p>Advances in artificial intelligence (AI) and Earth observation (EO) have transformed urban studies. This paper provides a commentary on how the AI-EO integration offers advancements in urban studies and applications. We conclude that AI will provide a deeper interpretation and autonomous identification of urban issues and the creation of customized urban designs. Open issues remain, especially in integrating diverse geospatial big data, data security, and developing a general analytical framework.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2024.10_ISPRS%20P&amp;RS-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2024.10_ISPRS%20P&amp;RS-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2024.10_ISPRS%20P&amp;RS-1400.webp"></source> <img src="/assets/img/publication_preview/2024.10_ISPRS%20P&amp;RS.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2024.10_ISPRS P&amp;RS.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Li2024" class="col-sm-8"> <div class="title">Beyond clouds: Seamless flood mapping using Harmonized Landsat and Sentinel-2 time series imagery and water occurrence data</div> <div class="author"> <em>Zhiwei Li</em>,¬†Shaofen Xu,¬†and¬†Qihao Weng</div> <div class="periodical"> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="periodical"> <span style="color: #B509AC;"><b>Featured in ‚ÄôSelect Landsat Publications‚Äô by NASA Landsat Science‚≠ê</b></span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.isprsjprs.2024.07.022" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://zhiweili.net/assets/pdf/2024.10_ISPRS%20P&amp;RS_Beyond%20clouds%EF%BC%9ASeamless%20flood%20mapping%20using%20Harmonized%20Landsat%20and%20Sentinel-2%20time%20series%20imagery%20and%20water%20occurrence%20data.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/dr-lizhiwei/SeamlessFloodMapper" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.isprsjprs.2024.07.022" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <span class="altmetric-embed" data-badge-type="2" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-popover="right" data-link-target="true" data-doi="10.1016/j.isprsjprs.2024.07.022"></span> </div> <div class="abstract hidden"> <p>Floods are among the most devastating natural disasters, posing significant risks to life, property, and infrastructure globally. Earth observation satellites provide data for continuous and extensive flood monitoring, yet limitations exist in the spatial completeness of monitoring using optical images due to cloud cover. Recent studies have developed gap-filling methods for reconstructing cloud-covered areas in water maps. However, these methods are not tailored for and validated in cloudy and rainy flooding scenarios with rapid water extent changes and limited clear-sky observations, leaving room for further improvements. This study investigated and developed a novel reconstruction method for time series flood extent mapping, supporting spatially seamless monitoring of flood extents. The proposed method first identified surface water from time series images using a fine-tuned large foundation model. Then, the cloud-covered areas in the water maps were reconstructed, adhering to the introduced submaximal stability assumption, on the basis of the prior water occurrence data in the Global Surface Water dataset. The reconstructed time series water maps were refined through spatiotemporal Markov random field modeling for the final delineation of flooding areas. The effectiveness of the proposed method was evaluated with Harmonized Landsat and Sentinel-2 datasets under varying cloud cover conditions, enabling seamless flood mapping at 2‚Äì3-day frequency and 30 m resolution. Experiments at four global sites confirmed the superiority of the proposed method. It achieved higher reconstruction accuracy with average F1-scores of 0.931 during floods and 0.903 before/after floods, outperforming the typical gap-filling method with average F1-scores of 0.871 and 0.772, respectively. Additionally, the maximum flood extent maps and flood duration maps, which were composed on the basis of the reconstructed water maps, were more accurate than those using the original cloud-contaminated water maps. The benefits of synthetic aperture radar images (e.g., Sentinel-1) for enhancing flood mapping under cloud cover conditions were also discussed. The method proposed in this paper provided an effective way for flood monitoring in cloudy and rainy scenarios, supporting emergency response and disaster management. The code and datasets used in this study have been made available online (https://github.com/dr-lizhiwei/SeamlessFloodMapper).</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2024.7_RSE-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2024.7_RSE-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2024.7_RSE-1400.webp"></source> <img src="/assets/img/publication_preview/2024.7_RSE.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2024.7_RSE.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Li2026" class="col-sm-8"> <div class="title">Learning spectral-indices-fused deep models for time-series land use and land cover mapping in cloud-prone areas: The case of Pearl River Delta</div> <div class="author"> <em>Zhiwei Li</em>,¬†Qihao Weng,¬†Yuhan Zhou,¬†Peng Dou,¬†and¬†Xiaoli Ding</div> <div class="periodical"> <em>Remote Sensing of Environment</em>, Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.rse.2024.114190" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://zhiweili.net/assets/pdf/2024.7_RSE_Learning%20spectral-indices-fused%20deep%20models%20for%20time-series%20land%20use%20and%20land%20cover%20mapping%20in%20cloud-prone%20areas.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.rse.2024.114190" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <span class="altmetric-embed" data-badge-type="2" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-popover="right" data-link-target="true" data-doi="10.1016/j.rse.2024.114190"></span> </div> <div class="abstract hidden"> <p>Mapping of highly dynamic changes in land use and land cover (LULC) can be hindered by various cloudy conditions with optical satellite images. These conditions result in discontinuities in high-temporal-density LULC mapping. In this paper, we developed an integrated time series mapping method to enhance the LULC mapping accuracy and frequency in cloud-prone areas by incorporating spectral-indices-fused deep models and time series reconstruction techniques. The proposed method first reconstructed cloud-contaminated pixels through time series filtering, during which the cloud masks initialized by a deep model were refined and updated during the reconstruction process. Then, the reconstructed time series images were fed into a spectral-indices-fused deep model trained on samples collected worldwide for classification. Finally, post-classification processing, including spatio-temporal majority filtering and time series refinement considering land‚Äìwater interactions, was conducted to enhance the LULC mapping accuracy and consistency. We applied the proposed method to the cloud- and rain-prone Pearl River Delta (i.e., Guangdong‚ÄìHong Kong‚ÄìMacao Greater Bay Area, GBA) and used time series Sentinel-2 images as the experimental data. The proposed method enabled seamless LULC mapping at a temporal frequency of 2‚Äì5 days, and the production of 10 m resolution annual LULC products in the GBA. The assessment yielded a mean overall accuracy of 87.01% for annual mapping in the four consecutive years of 2019‚Äì2022 and outperformed existing mainstream LULC products, including ESA WorldCover (83.98%), Esri Land Cover (85.26%), and Google Dynamic World (85.06%). Our assessment also reveals significant variations in LULC mapping accuracies with different cloud masks, thus underscoring their critical role in time series LULC mapping. The proposed method has the potential to generate seamless and near real-time maps for other regions in the world by using deep models trained on datasets collected globally. This method can provide high-quality LULC data sets at different time intervals for various land and water dynamics in cloud- and rain-prone regions. Notwithstanding the difficulties of obtaining high-quality LULC maps in cloud-prone areas, this paper provides a novel approach for the mapping of LULC dynamics and the provision of reliable annual LULC products.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2022.6_ISPRS%20P&amp;RS-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2022.6_ISPRS%20P&amp;RS-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2022.6_ISPRS%20P&amp;RS-1400.webp"></source> <img src="/assets/img/publication_preview/2022.6_ISPRS%20P&amp;RS.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2022.6_ISPRS P&amp;RS.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Li2022" class="col-sm-8"> <div class="title">Cloud and cloud shadow detection for optical satellite imagery: Features, algorithms, validation, and prospects</div> <div class="author"> <em>Zhiwei Li</em>,¬†Huanfeng Shen,¬†Qihao Weng,¬†Yuzhuo Zhang,¬†Peng Dou,¬†and¬†Liangpei Zhang</div> <div class="periodical"> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="periodical"> <span style="color: #B509AC;"><b>ESI Highly Cited PaperüèÜ Editor‚Äôs Article Choice of the Year 2022‚≠ê</b></span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://linkinghub.elsevier.com/retrieve/pii/S0924271622000934" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://zhiweili.net/assets/pdf/2022.6_ISPRS%20P&amp;RS_Cloud%20and%20cloud%20shadow%20detection%20for%20optical%20satellite%20imagery%EF%BC%9AFeatures,%20algorithms,%20validation,%20and%20prospects.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/dr-lizhiwei/OpenSICDR" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.isprsjprs.2022.03.020" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <span class="altmetric-embed" data-badge-type="2" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-popover="right" data-link-target="true" data-doi="10.1016/j.isprsjprs.2022.03.020"></span> </div> <div class="abstract hidden"> <p>The presence of clouds prevents optical satellite imaging systems from obtaining useful Earth observation information and negatively affects the processing and application of optical satellite images. Therefore, the detection of clouds and their accompanying shadows is an essential step in preprocessing optical satellite images and has emerged as a popular research topic in recent decades due to the interest in image time series analysis and remote sensing data mining. This review first analyzes the trends of the field, summarizes the progress and achievements in the cloud and cloud shadow detection methods in terms of features, algorithms, and validation of results, and then discusses existing problems, and provides our prospects at the end. We aim at identifying the emerging research trends and opportunities, while providing guidance for selecting the most suitable methods for coping with cloud contaminated problems faced by optical satellite images, an extremely important issue for remote sensing of cloudy and rainy areas. In the future, expected improvements in accuracy and generalizability, the combination of physical models and deep learning, as well as artificial intelligence and online big data processing platforms will be able to further promote processing efficiency and facilitate applications of image time series. In addition, this review collects the latest open-source tools and datasets for cloud and cloud shadow detection and launches an online project (Open Satellite Image Cloud Detection Resources, i.e., OpenSICDR) to share the latest research outputs (https://github.com/dr-lizhiwei/OpenSICDR).</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2021.12_JAG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2021.12_JAG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2021.12_JAG-1400.webp"></source> <img src="/assets/img/publication_preview/2021.12_JAG.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2021.12_JAG.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Dou2021" class="col-sm-8"> <div class="title">Time series remote sensing image classification framework using combination of deep learning and multiple classifiers system</div> <div class="author"> Peng Dou,¬†Huanfeng Shen,¬†<em>Zhiwei Li*</em>,¬†and¬†Xiaobin Guan</div> <div class="periodical"> <em>International Journal of Applied Earth Observation and Geoinformation</em>, Dec 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/pii/S0303243421001847" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://zhiweili.net/assets/pdf/2021.12_JAG_Time%20series%20remote%20sensing%20image%20classification%20framework%20using%20combination%20of%20deep%20learning%20and%20multiple%20classifiers%20system.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.jag.2021.102477" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <span class="altmetric-embed" data-badge-type="2" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-popover="right" data-link-target="true" data-doi="10.1016/j.jag.2021.102477"></span> </div> <div class="abstract hidden"> <p>Recently, time series image (TSI) has been reported to be an effective resource to mapping fine land use/land cover (LULC), and deep learning, in particular, has been gaining growing attention in this field. However, deep learning methods using single classifier need further improvement for accurate TSI classification owing to the 1D temporal properties and insufficient dense time series of the remote sensing images. To overcome such disadvantages, we proposed an innovative approach involving construction of TSI and combination of deep learning and multiple classifiers system (MCS). Firstly, we used a normalised difference index (NDI) to establish an NDIs-based TSI and then designed a framework consisting of a deep learning-based feature extractor and multiple classifiers system (MCS) based classification model to classify the TSI. With the new approach, our experiments were conducted on Landsat images located in two counties, Sutter and Kings in California, United States. The experimental results indicate that our proposed method achieves great progress on accuracy improvement and LULC mapping, outperforming classifications using comparative deep learning and non-deep learning methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2021.7_ISPRS%20P&amp;RS-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2021.7_ISPRS%20P&amp;RS-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2021.7_ISPRS%20P&amp;RS-1400.webp"></source> <img src="/assets/img/publication_preview/2021.7_ISPRS%20P&amp;RS.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2021.7_ISPRS P&amp;RS.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Zhang2021" class="col-sm-8"> <div class="title">Combined deep prior with low-rank tensor SVD for thick cloud removal in multitemporal images</div> <div class="author"> Qiang Zhang,¬†Qiangqiang Yuan*,¬†<em>Zhiwei Li*</em>,¬†Fujun Sun,¬†and¬†Liangpei Zhang</div> <div class="periodical"> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, Jul 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://linkinghub.elsevier.com/retrieve/pii/S0924271621001258" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://zhiweili.net/assets/pdf/2021.7_ISPRS%20P&amp;RS_Combined%20deep%20prior%20with%20low-rank%20tensor%20SVD%20for%20thick%20cloud%20removal%20in%20multitemporal%20images.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://drive.google.com/file/d/1LlvUKtUWAKoF6R0igbREwvP2Wfja9UBv/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.isprsjprs.2021.04.021" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <span class="altmetric-embed" data-badge-type="2" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-popover="right" data-link-target="true" data-doi="10.1016/j.isprsjprs.2021.04.021"></span> </div> <div class="abstract hidden"> <p>The thick cloud coverage phenomenon severely disturbs optical satellite observation missions (covering approximately 40‚Äì60% areas in the global scale). Therefore, the manner by which to eliminate thick cloud in remote sensing imagery is greatly significant and indispensable. In this study, we combine the deep spatio-temporal prior with low-rank tensor singular value decomposition (DP-LRTSVD) for thick cloud removal in multitemporal images. On the one hand, DP-LRTSVD utilizes the low-rank characteristic of multitemporal images via the third-order tensor SVD and completion. On the other hand, DP-LRTSVD employs the deep spatio-temporal feature expression ability by 3D convolutional neural network. The proposed framework can effectively eliminate thick cloud in multitemporal images through combining the model-driven and data-driven strategies. Moreover, DP-LRTSVD outperforms on thick cloud removal in the simulated and real multitemporal Sentinel-2/GF-1 experiments compared with model-driven or data-driven methods. In contrast with most methods that can only use a single reference image for thick cloud removal, the proposed method can simultaneously eliminate thick cloud in time-series images.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2019.8_RS-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2019.8_RS-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2019.8_RS-1400.webp"></source> <img src="/assets/img/publication_preview/2019.8_RS.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2019.8_RS.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Li2019" class="col-sm-8"> <div class="title">Thick cloud removal in high-resolution satellite images using stepwise radiometric adjustment and residual correction</div> <div class="author"> <em>Zhiwei Li</em>,¬†Huanfeng Shen,¬†Qing Cheng,¬†Wei Li,¬†and¬†Liangpei Zhang</div> <div class="periodical"> <em>Remote Sensing</em>, Aug 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.mdpi.com/2072-4292/11/16/1925" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://zhiweili.net/assets/pdf/2019.8_RS_Thick%20Cloud%20Removal%20in%20High-Resolution%20Satellite%20Images%20Using%20Stepwise%20Radiometric%20Adjustment%20and%20Residual%20Correction.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.3390/rs11161925" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <span class="altmetric-embed" data-badge-type="2" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-popover="right" data-link-target="true" data-doi="10.3390/rs11161925"></span> </div> <div class="abstract hidden"> <p>Cloud cover is a common problem in optical satellite imagery, which leads to missing information in images as well as a reduction in the data usability. In this paper, a thick cloud removal method based on stepwise radiometric adjustment and residual correction (SRARC) is proposed, which is aimed at effectively removing the clouds in high-resolution images for the generation of high-quality and spatially contiguous urban geographical maps. The basic idea of SRARC is that the complementary information in adjacent temporal satellite images can be utilized for the seamless recovery of cloud-contaminated areas in the target image after precise radiometric adjustment. To this end, the SRARC method first optimizes the given cloud mask of the target image based on superpixel segmentation, which is conducted to ensure that the labeled cloud boundaries go through homogeneous areas of the target image, to ensure a seamless reconstruction. Stepwise radiometric adjustment is then used to adjust the radiometric information of the complementary areas in the auxiliary image, step by step, and clouds in the target image can be removed by the replacement with the adjusted complementary areas. Finally, residual correction based on global optimization is used to further reduce the radiometric differences between the recovered areas and the cloud-free areas. The final cloud removal results are then generated. High-resolution images with different spatial resolutions and land-cover change patterns were used in both simulated and real-data cloud removal experiments. The results suggest that SRARC can achieve a better performance than the other compared methods, due to the superiority of the radiometric adjustment and spatial detail preservation. SRARC is thus a promising approach that has the potential for routine use, to support applications based on high-resolution satellite images.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2019.4_ISPRS%20P&amp;RS-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2019.4_ISPRS%20P&amp;RS-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2019.4_ISPRS%20P&amp;RS-1400.webp"></source> <img src="/assets/img/publication_preview/2019.4_ISPRS%20P&amp;RS.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2019.4_ISPRS P&amp;RS.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Li2020" class="col-sm-8"> <div class="title">Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors</div> <div class="author"> <em>Zhiwei Li</em>,¬†Huanfeng Shen,¬†Qing Cheng,¬†Yuhao Liu,¬†Shucheng You,¬†and¬†Zongyi He</div> <div class="periodical"> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, Apr 2019 </div> <div class="periodical"> </div> <div class="periodical"> <span style="color: #B509AC;"><b>ESI Highly Cited PaperüèÜ</b></span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0924271619300565" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://zhiweili.net/assets/pdf/2019.4_ISPRS%20P&amp;RS_Deep%20learning%20based%20cloud%20detection%20for%20medium%20and%20high%20resolution%20remote%20sensing%20images%20of%20different%20sensors.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/dr-lizhiwei/MSCFF" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project</a> <a href="https://github.com/dr-lizhiwei/HRC_WHU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.isprsjprs.2019.02.017" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <span class="altmetric-embed" data-badge-type="2" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-popover="right" data-link-target="true" data-doi="10.1016/j.isprsjprs.2019.02.017"></span> </div> <div class="abstract hidden"> <p>Cloud detection is an important preprocessing step for the precise application of optical satellite imagery. In this paper, we propose a deep learning based cloud detection method named multi-scale convolutional feature fusion (MSCFF) for remote sensing images of different sensors. In the network architecture of MSCFF, the symmetric encoder-decoder module, which provides both local and global context by densifying feature maps with trainable convolutional filter banks, is utilized to extract multi-scale and high-level spatial features. The feature maps of multiple scales are then up-sampled and concatenated, and a novel multi-scale feature fusion module is designed to fuse the features of different scales for the output. The two output feature maps of the network are cloud and cloud shadow maps, which are in turn fed to binary classifiers outside the model to obtain the final cloud and cloud shadow mask. The MSCFF method was validated on hundreds of globally distributed optical satellite images, with spatial resolutions ranging from 0.5 to 50 m, including Landsat-5/7/8, Gaofen-1/2/4, Sentinel-2, Ziyuan-3, CBERS-04, Huanjing-1, and collected high-resolution images exported from Google Earth. The experimental results show that MSCFF achieves a higher accuracy than the traditional rule-based cloud detection methods and the state-of-the-art deep learning models, especially in bright surface covered areas. The effectiveness of MSCFF means that it has great promise for the practical application of cloud detection for multiple types of medium and high-resolution remote sensing images. Our established global high-resolution cloud detection validation dataset has been made available online (http://sendimage.whu.edu.cn/en/mscff/).</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/2017.3_RSE-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/2017.3_RSE-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/2017.3_RSE-1400.webp"></source> <img src="/assets/img/publication_preview/2017.3_RSE.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="2017.3_RSE.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Li2017" class="col-sm-8"> <div class="title">Multi-feature combined cloud and cloud shadow detection in GaoFen-1 wide field of view imagery</div> <div class="author"> <em>Zhiwei Li</em>,¬†Huanfeng Shen,¬†Huifang Li,¬†Guisong Xia,¬†Paolo Gamba,¬†and¬†Liangpei Zhang</div> <div class="periodical"> <em>Remote Sensing of Environment</em>, Mar 2017 </div> <div class="periodical"> </div> <div class="periodical"> <span style="color: #B509AC;"><b>ESI Highly Cited PaperüèÜ</b></span> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S003442571730038X" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://zhiweili.net/assets/pdf/2017.3_RSE_Multi-feature%20combined%20cloud%20and%20cloud%20shadow%20detection%20in%20GaoFen-1%20wide%20field%20of%20view%20imagery.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/dr-lizhiwei/MFC" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Project</a> <a href="https://github.com/dr-lizhiwei/GF1_WHU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.rse.2017.01.026" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <span class="altmetric-embed" data-badge-type="2" data-hide-no-mentions="true" data-hide-less-than="1" data-badge-popover="right" data-link-target="true" data-doi="10.1016/j.rse.2017.01.026"></span> </div> <div class="abstract hidden"> <p>The wide field of view (WFV) imaging system onboard the Chinese GaoFen-1 (GF-1) optical satellite has a 16-m resolution and four-day revisit cycle for large-scale Earth observation. The advantages of the high temporal-spatial resolution and the wide field of view make the GF-1 WFV imagery very popular. However, cloud cover is an inevitable problem in GF-1 WFV imagery, which influences its precise application. Accurate cloud and cloud shadow detection in GF-1 WFV imagery is quite difficult due to the fact that there are only three visible bands and one near-infrared band. In this paper, an automatic multi-feature combined (MFC) method is proposed for cloud and cloud shadow detection in GF-1 WFV imagery. The MFC algorithm first implements threshold segmentation based on the spectral features and mask refinement based on guided filtering to generate a preliminary cloud mask. The geometric features are then used in combination with the texture features to improve the cloud detection results and produce the final cloud mask. Finally, the cloud shadow mask can be acquired by means of the cloud and shadow matching and follow-up correction process. The method was validated using 108 globally distributed scenes. The results indicate that MFC performs well under most conditions, and the average overall accuracy of MFC cloud detection is as high as 96.8%. In the contrastive analysis with the official provided cloud fractions, MFC shows a significant improvement in cloud fraction estimation, and achieves a high accuracy for the cloud and cloud shadow detection in the GF-1 WFV imagery with fewer spectral bands. The proposed method could be used as a preprocessing step in the future to monitor land-cover change, and it could also be easily extended to other optical satellite imagery which has a similar spectral setting. The global validation dataset and the software tool used in this study have been made available online (http://sendimage.whu.edu.cn/en/mfc/).</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container text-center"> ¬© Copyright 2025 Zhiwei Li. Last updated: October 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-WN6SN30FQ9"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-WN6SN30FQ9");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>